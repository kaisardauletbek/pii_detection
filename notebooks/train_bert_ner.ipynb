{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/kd/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/kd/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "import torch\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "from datasets import Dataset, features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentencepiece\n",
    "# import data handling functions from utils (a diretory above)\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.data_handler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/kd/Documents/pii_detection/data/'\n",
    "OUTPUT_DIR = '/home/kd/Documents/pii_detection/models/'\n",
    "# TRAINING_MODEL_PATH = 'dslim/bert-base-NER'\n",
    "TRAINING_MODEL_PATH = 'dslim/bert-large-NER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "# example = \"My name is Wolfgang and I live in Berlin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original datapoints:  6807\n",
      "external datapoints:  4434\n",
      "moredata datapoints:  2000\n",
      "combined:  7333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1fcc96a0ef42b59f7b4a67b46bc0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/7333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "data = do_load_data(DATA_PATH)\n",
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "target = [\n",
    "    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n",
    "    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n",
    "    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)\n",
    "\n",
    "ds = do_hf_dataset(tokenizer, label2id, data, TRAINING_MAX_LENGTH=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import recall_score, precision_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(p, all_labels):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "    \n",
    "    results = {\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': f1_score\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dslim/bert-large-NER and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 1024]) in the checkpoint and torch.Size([13, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    TRAINING_MODEL_PATH,\n",
    "    num_labels=len(all_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I actually chose to not use any validation set. This is only for the model I use for submission.\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR, \n",
    "    fp16=True,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=5,\n",
    "    gradient_accumulation_steps=2,\n",
    "    report_to=\"none\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    do_eval=False,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=20,\n",
    "    lr_scheduler_type='cosine',\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=args, \n",
    "    train_dataset=ds,\n",
    "    data_collator=collator, \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=partial(compute_metrics, all_labels=all_labels),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845071d2addb41c79202a933e8d30a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4989, 'learning_rate': 1.8181818181818183e-06, 'epoch': 0.03}\n",
      "{'loss': 0.8248, 'learning_rate': 3.6363636363636366e-06, 'epoch': 0.05}\n",
      "{'loss': 0.1712, 'learning_rate': 5.4545454545454545e-06, 'epoch': 0.08}\n",
      "{'loss': 0.0863, 'learning_rate': 7.272727272727273e-06, 'epoch': 0.11}\n",
      "{'loss': 0.0643, 'learning_rate': 9.090909090909091e-06, 'epoch': 0.14}\n",
      "{'loss': 0.0584, 'learning_rate': 1.0909090909090909e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0385, 'learning_rate': 1.2727272727272728e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0389, 'learning_rate': 1.4545454545454546e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0371, 'learning_rate': 1.6363636363636366e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0323, 'learning_rate': 1.8181818181818182e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0353, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0276, 'learning_rate': 1.9994960334973702e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0301, 'learning_rate': 1.997984641953951e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0257, 'learning_rate': 1.995467348751164e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0213, 'learning_rate': 1.9919466911519112e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0244, 'learning_rate': 1.9874262177431877e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0225, 'learning_rate': 1.9819104848593418e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0219, 'learning_rate': 1.9754050519895946e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0199, 'learning_rate': 1.9679164761744496e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0194, 'learning_rate': 1.959452305396633e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0257, 'learning_rate': 1.9500210709732343e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0215, 'learning_rate': 1.9396322789567087e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0229, 'learning_rate': 1.9282964005534144e-05, 'epoch': 0.63}\n",
      "{'loss': 0.016, 'learning_rate': 1.916024861569338e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0259, 'learning_rate': 1.9028300308936472e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0237, 'learning_rate': 1.8887252080316782e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0187, 'learning_rate': 1.8737246096999272e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0178, 'learning_rate': 1.8578433554965513e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0176, 'learning_rate': 1.8410974526618275e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0149, 'learning_rate': 1.8235037799439252e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0169, 'learning_rate': 1.805080070586261e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0184, 'learning_rate': 1.7858448944535753e-05, 'epoch': 0.87}\n",
      "{'loss': 0.016, 'learning_rate': 1.7658176393147548e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0168, 'learning_rate': 1.745018491301258e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0151, 'learning_rate': 1.7234684145608497e-05, 'epoch': 0.95}\n",
      "{'loss': 0.0148, 'learning_rate': 1.7011891301271423e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0149, 'learning_rate': 1.6782030940262494e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0154, 'learning_rate': 1.6545334746426184e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0157, 'learning_rate': 1.630204129366847e-05, 'epoch': 1.06}\n",
      "{'loss': 0.011, 'learning_rate': 1.6052395805490354e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0149, 'learning_rate': 1.5796649907818987e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0098, 'learning_rate': 1.5535061375385588e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0181, 'learning_rate': 1.5267893871905796e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0115, 'learning_rate': 1.4995416684324304e-05, 'epoch': 1.2}\n",
      "{'loss': 0.012, 'learning_rate': 1.4717904451391651e-05, 'epoch': 1.23}\n",
      "{'loss': 0.014, 'learning_rate': 1.4435636886846775e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0122, 'learning_rate': 1.41488984974843e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0111, 'learning_rate': 1.3857978296390733e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0136, 'learning_rate': 1.3563169511638657e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0133, 'learning_rate': 1.326476929073246e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0122, 'learning_rate': 1.2963078401103575e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0123, 'learning_rate': 1.2658400926957044e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0083, 'learning_rate': 1.2351043962775017e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0142, 'learning_rate': 1.204131730378609e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0127, 'learning_rate': 1.1729533133712472e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0132, 'learning_rate': 1.1416005710109693e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0124, 'learning_rate': 1.1101051047616054e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0125, 'learning_rate': 1.0784986599431049e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0133, 'learning_rate': 1.0468130937343787e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0144, 'learning_rate': 1.0150803430633998e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0144, 'learning_rate': 9.833323924169164e-06, 'epoch': 1.66}\n",
      "{'loss': 0.0123, 'learning_rate': 9.516012416022346e-06, 'epoch': 1.69}\n",
      "{'loss': 0.0109, 'learning_rate': 9.199188734935555e-06, 'epoch': 1.72}\n",
      "{'loss': 0.0126, 'learning_rate': 8.883172217953812e-06, 'epoch': 1.75}\n",
      "{'loss': 0.0128, 'learning_rate': 8.56828138855478e-06, 'epoch': 1.77}\n",
      "{'loss': 0.0117, 'learning_rate': 8.25483363559847e-06, 'epoch': 1.8}\n",
      "{'loss': 0.0136, 'learning_rate': 7.94314489342051e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0167, 'learning_rate': 7.633529323391511e-06, 'epoch': 1.85}\n",
      "{'loss': 0.0106, 'learning_rate': 7.326298997263444e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0125, 'learning_rate': 7.021763582622234e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0091, 'learning_rate': 6.720230030763573e-06, 'epoch': 1.94}\n",
      "{'loss': 0.0134, 'learning_rate': 6.422002267306562e-06, 'epoch': 1.96}\n",
      "{'loss': 0.0137, 'learning_rate': 6.127380885857078e-06, 'epoch': 1.99}\n",
      "{'loss': 0.0092, 'learning_rate': 5.836662845029544e-06, 'epoch': 2.02}\n",
      "{'loss': 0.0068, 'learning_rate': 5.550141169132528e-06, 'epoch': 2.04}\n",
      "{'loss': 0.0123, 'learning_rate': 5.268104652819894e-06, 'epoch': 2.07}\n",
      "{'loss': 0.0083, 'learning_rate': 4.990837570005124e-06, 'epoch': 2.1}\n",
      "{'loss': 0.0098, 'learning_rate': 4.718619387332254e-06, 'epoch': 2.13}\n",
      "{'loss': 0.0118, 'learning_rate': 4.45172448249223e-06, 'epoch': 2.15}\n",
      "{'loss': 0.0097, 'learning_rate': 4.190421867668584e-06, 'epoch': 2.18}\n",
      "{'loss': 0.0054, 'learning_rate': 3.93497491839115e-06, 'epoch': 2.21}\n",
      "{'loss': 0.0097, 'learning_rate': 3.685641108071205e-06, 'epoch': 2.24}\n",
      "{'loss': 0.0101, 'learning_rate': 3.4426717484854944e-06, 'epoch': 2.26}\n",
      "{'loss': 0.0089, 'learning_rate': 3.206311736470811e-06, 'epoch': 2.29}\n",
      "{'loss': 0.0109, 'learning_rate': 2.976799307084385e-06, 'epoch': 2.32}\n",
      "{'loss': 0.0087, 'learning_rate': 2.7543657934789194e-06, 'epoch': 2.34}\n",
      "{'loss': 0.0114, 'learning_rate': 2.5392353937342494e-06, 'epoch': 2.37}\n",
      "{'loss': 0.0114, 'learning_rate': 2.3316249448807102e-06, 'epoch': 2.4}\n",
      "{'loss': 0.0115, 'learning_rate': 2.1317437043419442e-06, 'epoch': 2.43}\n",
      "{'loss': 0.0079, 'learning_rate': 1.939793139017416e-06, 'epoch': 2.45}\n",
      "{'loss': 0.0096, 'learning_rate': 1.7559667222173037e-06, 'epoch': 2.48}\n",
      "{'loss': 0.0063, 'learning_rate': 1.5804497386543272e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0101, 'learning_rate': 1.4134190976892125e-06, 'epoch': 2.54}\n",
      "{'loss': 0.0098, 'learning_rate': 1.2550431550178732e-06, 'epoch': 2.56}\n",
      "{'loss': 0.0089, 'learning_rate': 1.1054815429801702e-06, 'epoch': 2.59}\n",
      "{'loss': 0.0066, 'learning_rate': 9.648850096611929e-07, 'epoch': 2.62}\n",
      "{'loss': 0.0085, 'learning_rate': 8.333952669473022e-07, 'epoch': 2.64}\n",
      "{'loss': 0.0128, 'learning_rate': 7.111448476900318e-07, 'epoch': 2.67}\n",
      "{'loss': 0.0064, 'learning_rate': 5.982569721218557e-07, 'epoch': 2.7}\n",
      "{'loss': 0.0095, 'learning_rate': 4.948454236584555e-07, 'epoch': 2.73}\n",
      "{'loss': 0.0096, 'learning_rate': 4.0101443421264985e-07, 'epoch': 2.75}\n",
      "{'loss': 0.0084, 'learning_rate': 3.1685857913561937e-07, 'epoch': 2.78}\n",
      "{'loss': 0.0115, 'learning_rate': 2.424626818912812e-07, 'epoch': 2.81}\n",
      "{'loss': 0.0084, 'learning_rate': 1.7790172855992493e-07, 'epoch': 2.84}\n",
      "{'loss': 0.0111, 'learning_rate': 1.2324079225726426e-07, 'epoch': 2.86}\n",
      "{'loss': 0.0124, 'learning_rate': 7.853496754509637e-08, 'epoch': 2.89}\n",
      "{'loss': 0.0073, 'learning_rate': 4.38293148996749e-08, 'epoch': 2.92}\n",
      "{'loss': 0.0096, 'learning_rate': 1.9158815293771617e-08, 'epoch': 2.94}\n",
      "{'loss': 0.0066, 'learning_rate': 4.548334938196952e-09, 'epoch': 2.97}\n",
      "{'train_runtime': 2226.734, 'train_samples_per_second': 9.879, 'train_steps_per_second': 0.988, 'train_loss': 0.047519769893960014, 'epoch': 3.0}\n",
      "CPU times: user 24min 24s, sys: 12min 42s, total: 37min 7s\n",
      "Wall time: 37min 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2199, training_loss=0.047519769893960014, metrics={'train_runtime': 2226.734, 'train_samples_per_second': 9.879, 'train_steps_per_second': 0.988, 'train_loss': 0.047519769893960014, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/kd/Documents/pii_detection/models//bert_large_ner/tokenizer_config.json',\n",
       " '/home/kd/Documents/pii_detection/models//bert_large_ner/special_tokens_map.json',\n",
       " '/home/kd/Documents/pii_detection/models//bert_large_ner/vocab.txt',\n",
       " '/home/kd/Documents/pii_detection/models//bert_large_ner/added_tokens.json',\n",
       " '/home/kd/Documents/pii_detection/models//bert_large_ner/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(OUTPUT_DIR + \"/bert_large_ner\")\n",
    "tokenizer.save_pretrained(OUTPUT_DIR + \"/bert_large_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
