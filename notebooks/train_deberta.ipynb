{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kd/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/kd/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/kd/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "import torch\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "from datasets import Dataset, features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentencepiece\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.data_handler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/kd/Documents/pii_detection/data/'\n",
    "OUTPUT_DIR = '/home/kd/Documents/pii_detection/models/'\n",
    "TRAINING_MODEL_PATH = \"microsoft/deberta-v3-large\"\n",
    "TRAINING_MAX_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original datapoints:  6807\n",
      "external datapoints:  4434\n",
      "mixtral  2355\n",
      "combined:  9688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/kd/anaconda3/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2753cb8c89d9449ebb12daae2acde78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/9688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "data = do_load_data(DATA_PATH)\n",
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "target = [\n",
    "    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n",
    "    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n",
    "    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)\n",
    "\n",
    "ds = do_hf_dataset(tokenizer, label2id, data, TRAINING_MAX_LENGTH=TRAINING_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, label2id, max_length):\n",
    "\n",
    "    # rebuild text from tokens\n",
    "    text = []\n",
    "    labels = []\n",
    "\n",
    "    for t, l, ws in zip(\n",
    "        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n",
    "    ):\n",
    "        text.append(t)\n",
    "        labels.extend([l] * len(t))\n",
    "\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "\n",
    "    # actual tokenization\n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    text = \"\".join(text)\n",
    "    token_labels = []\n",
    "\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # CLS token\n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            token_labels.append(label2id[\"O\"])\n",
    "            continue\n",
    "\n",
    "        # case when token starts with whitespace\n",
    "        if text[start_idx].isspace():\n",
    "            start_idx += 1\n",
    "\n",
    "        token_labels.append(label2id[labels[start_idx]])\n",
    "\n",
    "    length = len(tokenized.input_ids)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['full_text', 'document', 'tokens', 'trailing_whitespace', 'provided_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels', 'length'],\n",
       "    num_rows: 9688\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import recall_score, precision_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(p, all_labels):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "    \n",
    "    results = {\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': f1_score\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    TRAINING_MODEL_PATH,\n",
    "    num_labels=len(all_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I actually chose to not use any validation set. This is only for the model I use for submission.\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR, \n",
    "    fp16=True,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    evaluation_strategy=\"no\",\n",
    "    do_eval=False,\n",
    "    save_total_limit=1,\n",
    "    logging_dir='/home/kd/Documents/pii_detection/logs/',\n",
    "    logging_steps=20,\n",
    "    lr_scheduler_type='cosine',\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=args, \n",
    "    train_dataset=ds,\n",
    "    data_collator=collator, \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=partial(compute_metrics, all_labels=all_labels),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84a421bb52b4c28bd85a44e6612ec7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2044, 'learning_rate': 2.2008253094910594e-07, 'epoch': 0.0}\n",
      "{'loss': 2.1504, 'learning_rate': 4.951856946354883e-07, 'epoch': 0.01}\n",
      "{'loss': 1.7236, 'learning_rate': 7.702888583218708e-07, 'epoch': 0.01}\n",
      "{'loss': 1.0526, 'learning_rate': 1.0453920220082531e-06, 'epoch': 0.02}\n",
      "{'loss': 0.4039, 'learning_rate': 1.3204951856946355e-06, 'epoch': 0.02}\n",
      "{'loss': 0.2232, 'learning_rate': 1.595598349381018e-06, 'epoch': 0.02}\n",
      "{'loss': 0.1771, 'learning_rate': 1.8707015130674006e-06, 'epoch': 0.03}\n",
      "{'loss': 0.1334, 'learning_rate': 2.145804676753783e-06, 'epoch': 0.03}\n",
      "{'loss': 0.1336, 'learning_rate': 2.4209078404401654e-06, 'epoch': 0.04}\n",
      "{'loss': 0.0827, 'learning_rate': 2.6960110041265474e-06, 'epoch': 0.04}\n",
      "{'loss': 0.067, 'learning_rate': 2.97111416781293e-06, 'epoch': 0.05}\n",
      "{'loss': 0.0756, 'learning_rate': 3.246217331499312e-06, 'epoch': 0.05}\n",
      "{'loss': 0.0473, 'learning_rate': 3.521320495185695e-06, 'epoch': 0.05}\n",
      "{'loss': 0.0465, 'learning_rate': 3.796423658872077e-06, 'epoch': 0.06}\n",
      "{'loss': 0.0472, 'learning_rate': 4.07152682255846e-06, 'epoch': 0.06}\n",
      "{'loss': 0.0336, 'learning_rate': 4.346629986244843e-06, 'epoch': 0.07}\n",
      "{'loss': 0.024, 'learning_rate': 4.621733149931225e-06, 'epoch': 0.07}\n",
      "{'loss': 0.035, 'learning_rate': 4.8968363136176075e-06, 'epoch': 0.07}\n",
      "{'loss': 0.024, 'learning_rate': 5.17193947730399e-06, 'epoch': 0.08}\n",
      "{'loss': 0.0239, 'learning_rate': 5.447042640990372e-06, 'epoch': 0.08}\n",
      "{'loss': 0.026, 'learning_rate': 5.722145804676754e-06, 'epoch': 0.09}\n",
      "{'loss': 0.0137, 'learning_rate': 5.997248968363136e-06, 'epoch': 0.09}\n",
      "{'loss': 0.0188, 'learning_rate': 6.27235213204952e-06, 'epoch': 0.09}\n",
      "{'loss': 0.02, 'learning_rate': 6.547455295735902e-06, 'epoch': 0.1}\n",
      "{'loss': 0.012, 'learning_rate': 6.822558459422284e-06, 'epoch': 0.1}\n",
      "{'loss': 0.0399, 'learning_rate': 7.097661623108666e-06, 'epoch': 0.11}\n",
      "{'loss': 0.0218, 'learning_rate': 7.372764786795049e-06, 'epoch': 0.11}\n",
      "{'loss': 0.0132, 'learning_rate': 7.647867950481432e-06, 'epoch': 0.12}\n",
      "{'loss': 0.0152, 'learning_rate': 7.922971114167813e-06, 'epoch': 0.12}\n",
      "{'loss': 0.0083, 'learning_rate': 8.198074277854196e-06, 'epoch': 0.12}\n",
      "{'loss': 0.0051, 'learning_rate': 8.473177441540578e-06, 'epoch': 0.13}\n",
      "{'loss': 0.0104, 'learning_rate': 8.748280605226961e-06, 'epoch': 0.13}\n",
      "{'loss': 0.0291, 'learning_rate': 9.023383768913342e-06, 'epoch': 0.14}\n",
      "{'loss': 0.0162, 'learning_rate': 9.298486932599725e-06, 'epoch': 0.14}\n",
      "{'loss': 0.0176, 'learning_rate': 9.573590096286108e-06, 'epoch': 0.14}\n",
      "{'loss': 0.0097, 'learning_rate': 9.848693259972491e-06, 'epoch': 0.15}\n",
      "{'loss': 0.0124, 'learning_rate': 1.0123796423658874e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0057, 'learning_rate': 1.0398899587345255e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0173, 'learning_rate': 1.0674002751031638e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0082, 'learning_rate': 1.094910591471802e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0115, 'learning_rate': 1.1224209078404402e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0059, 'learning_rate': 1.1499312242090786e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0309, 'learning_rate': 1.1774415405777167e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0091, 'learning_rate': 1.204951856946355e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0161, 'learning_rate': 1.2324621733149933e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0115, 'learning_rate': 1.2599724896836314e-05, 'epoch': 0.19}\n",
      "{'loss': 0.009, 'learning_rate': 1.2874828060522697e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0085, 'learning_rate': 1.314993122420908e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0128, 'learning_rate': 1.3425034387895461e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0125, 'learning_rate': 1.3700137551581845e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0075, 'learning_rate': 1.3975240715268225e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0133, 'learning_rate': 1.425034387895461e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0179, 'learning_rate': 1.4525447042640992e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0124, 'learning_rate': 1.4800550206327373e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0232, 'learning_rate': 1.5075653370013756e-05, 'epoch': 0.23}\n",
      "{'loss': 0.01, 'learning_rate': 1.535075653370014e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0084, 'learning_rate': 1.5625859697386522e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0162, 'learning_rate': 1.5900962861072903e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0117, 'learning_rate': 1.6176066024759288e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0175, 'learning_rate': 1.645116918844567e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0083, 'learning_rate': 1.672627235213205e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0166, 'learning_rate': 1.700137551581843e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0175, 'learning_rate': 1.7276478679504815e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0032, 'learning_rate': 1.75515818431912e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0044, 'learning_rate': 1.782668500687758e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0037, 'learning_rate': 1.8101788170563962e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0038, 'learning_rate': 1.8376891334250347e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0147, 'learning_rate': 1.8651994497936728e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0105, 'learning_rate': 1.892709766162311e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0096, 'learning_rate': 1.920220082530949e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0106, 'learning_rate': 1.9477303988995875e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0184, 'learning_rate': 1.975240715268226e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0149, 'learning_rate': 1.9999998845890367e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0049, 'learning_rate': 1.9999860353056524e-05, 'epoch': 0.31}\n",
      "{'loss': 0.017, 'learning_rate': 1.9999491041958627e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0098, 'learning_rate': 1.9998890921121156e-05, 'epoch': 0.31}\n",
      "{'loss': 0.007, 'learning_rate': 1.99980600043962e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0117, 'learning_rate': 1.9996998310963098e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0073, 'learning_rate': 1.9995705865328018e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0013, 'learning_rate': 1.999418269732338e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0074, 'learning_rate': 1.9992428842107175e-05, 'epoch': 0.33}\n",
      "{'loss': 0.005, 'learning_rate': 1.999044434016215e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0175, 'learning_rate': 1.998822923729488e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0051, 'learning_rate': 1.9985783584634688e-05, 'epoch': 0.35}\n",
      "{'loss': 0.017, 'learning_rate': 1.99831074386325e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0011, 'learning_rate': 1.9980200861059514e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0043, 'learning_rate': 1.9977063919005785e-05, 'epoch': 0.36}\n",
      "{'loss': 0.005, 'learning_rate': 1.9973696684878675e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0127, 'learning_rate': 1.9970099236401188e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0052, 'learning_rate': 1.9966271656610162e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0057, 'learning_rate': 1.996221403385436e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0137, 'learning_rate': 1.9957926461792438e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0097, 'learning_rate': 1.995340903939077e-05, 'epoch': 0.38}\n",
      "{'loss': 0.012, 'learning_rate': 1.9948661870921175e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0062, 'learning_rate': 1.99436850659585e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0084, 'learning_rate': 1.9938478739378094e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0109, 'learning_rate': 1.9933043011353167e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0102, 'learning_rate': 1.9927378007351997e-05, 'epoch': 0.4}\n",
      "{'loss': 0.015, 'learning_rate': 1.9921483858135055e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0086, 'learning_rate': 1.991536069975196e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0059, 'learning_rate': 1.9909008673538374e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0018, 'learning_rate': 1.9902427926112707e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0029, 'learning_rate': 1.989561860937275e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0023, 'learning_rate': 1.9888580880492165e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0021, 'learning_rate': 1.988131490191686e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0057, 'learning_rate': 1.9873820841361226e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0031, 'learning_rate': 1.9866098871804288e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0032, 'learning_rate': 1.98581491714857e-05, 'epoch': 0.45}\n",
      "{'loss': 0.008, 'learning_rate': 1.984997192390162e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0144, 'learning_rate': 1.9841567317800493e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0035, 'learning_rate': 1.9832935547178693e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0025, 'learning_rate': 1.982407681127603e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0184, 'learning_rate': 1.9814991314571165e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0032, 'learning_rate': 1.9805679266776884e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0067, 'learning_rate': 1.9796140882835252e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0041, 'learning_rate': 1.978637638291268e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0078, 'learning_rate': 1.9776385992394797e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0014, 'learning_rate': 1.9766169941881287e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0085, 'learning_rate': 1.9755728467180546e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0067, 'learning_rate': 1.9745061809304248e-05, 'epoch': 0.5}\n",
      "{'loss': 0.007, 'learning_rate': 1.9734170214461776e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0025, 'learning_rate': 1.9723053934054536e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0029, 'learning_rate': 1.971171322467018e-05, 'epoch': 0.51}\n",
      "{'loss': 0.011, 'learning_rate': 1.9700148348076632e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0015, 'learning_rate': 1.9688359571216106e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0011, 'learning_rate': 1.9676347166198898e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0074, 'learning_rate': 1.9664111410297126e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0052, 'learning_rate': 1.9651652585938333e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0325, 'learning_rate': 1.9638970980698952e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0095, 'learning_rate': 1.962606688729768e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0057, 'learning_rate': 1.961294060358873e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0029, 'learning_rate': 1.9599592432554918e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0133, 'learning_rate': 1.9586022682300728e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0015, 'learning_rate': 1.957223166604515e-05, 'epoch': 0.55}\n",
      "{'loss': 0.002, 'learning_rate': 1.9558219702114467e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0005, 'learning_rate': 1.9543987113934923e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0005, 'learning_rate': 1.952953423002523e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0138, 'learning_rate': 1.9514861383989002e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0174, 'learning_rate': 1.949996891450706e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0022, 'learning_rate': 1.9484857165329592e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0056, 'learning_rate': 1.946952648526825e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0043, 'learning_rate': 1.945397722818806e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0011, 'learning_rate': 1.94382097529993e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0114, 'learning_rate': 1.9422224423649164e-05, 'epoch': 0.59}\n",
      "{'loss': 0.006, 'learning_rate': 1.9406021609113414e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0042, 'learning_rate': 1.938960168338782e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0153, 'learning_rate': 1.9372965025479544e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0045, 'learning_rate': 1.935611201939841e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0078, 'learning_rate': 1.9339043054147997e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0049, 'learning_rate': 1.932175852371671e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0009, 'learning_rate': 1.930425882706865e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0075, 'learning_rate': 1.928654436813442e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0044, 'learning_rate': 1.926861555580179e-05, 'epoch': 0.63}\n",
      "{'loss': 0.001, 'learning_rate': 1.9250472803906276e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0017, 'learning_rate': 1.9232116531221577e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0013, 'learning_rate': 1.9213547161449904e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0049, 'learning_rate': 1.919476512321222e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0019, 'learning_rate': 1.9175770850038323e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0082, 'learning_rate': 1.915656478035685e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0148, 'learning_rate': 1.913714735748515e-05, 'epoch': 0.66}\n",
      "{'loss': 0.006, 'learning_rate': 1.9117519029619082e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0068, 'learning_rate': 1.9097680249822624e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0074, 'learning_rate': 1.9077631476017436e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0043, 'learning_rate': 1.9057373170972297e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0151, 'learning_rate': 1.9036905802292427e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0036, 'learning_rate': 1.9016229842408664e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0067, 'learning_rate': 1.8995345768566594e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0111, 'learning_rate': 1.897425406281552e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0048, 'learning_rate': 1.895295521199732e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0014, 'learning_rate': 1.893144970773525e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0062, 'learning_rate': 1.8909738046422545e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0045, 'learning_rate': 1.8887820729211014e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0044, 'learning_rate': 1.886569826199942e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0012, 'learning_rate': 1.884337115542185e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0039, 'learning_rate': 1.8820839924835896e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0191, 'learning_rate': 1.8798105090310777e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0103, 'learning_rate': 1.877516717661532e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0029, 'learning_rate': 1.8752026713205865e-05, 'epoch': 0.73}\n",
      "{'loss': 0.001, 'learning_rate': 1.872868423421403e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0047, 'learning_rate': 1.8705140278434384e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0024, 'learning_rate': 1.868139538931202e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0041, 'learning_rate': 1.8657450114929993e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0034, 'learning_rate': 1.86333050079967e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0106, 'learning_rate': 1.860896062583308e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0079, 'learning_rate': 1.858441753035979e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0018, 'learning_rate': 1.855967628808421e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0116, 'learning_rate': 1.8534737470087363e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0027, 'learning_rate': 1.8509601652010773e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0007, 'learning_rate': 1.848426941404312e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0026, 'learning_rate': 1.845874134090689e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0025, 'learning_rate': 1.8433018021844865e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0106, 'learning_rate': 1.8407100050606524e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0024, 'learning_rate': 1.8380988025434336e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0123, 'learning_rate': 1.835468254904994e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0051, 'learning_rate': 1.8328184228640266e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0006, 'learning_rate': 1.8301493675843484e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0066, 'learning_rate': 1.82746115067349e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0031, 'learning_rate': 1.8247538341812738e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0032, 'learning_rate': 1.8220274805983828e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0094, 'learning_rate': 1.8192821528549146e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0016, 'learning_rate': 1.8165179143189337e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0061, 'learning_rate': 1.8137348287950044e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0099, 'learning_rate': 1.8109329605227208e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1837\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1840\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1843\u001b[0m ):\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2682\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2682\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   2684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2685\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2707\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2705\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2706\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2707\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   2708\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2709\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:687\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/utils/operations.py:675\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1418\u001b[0m, in \u001b[0;36mDebertaV2ForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1416\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1418\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeberta(\n\u001b[1;32m   1419\u001b[0m     input_ids,\n\u001b[1;32m   1420\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1421\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1422\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1423\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1424\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1425\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1426\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1427\u001b[0m )\n\u001b[1;32m   1429\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1431\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1084\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1076\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1077\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1078\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1082\u001b[0m )\n\u001b[0;32m-> 1084\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1085\u001b[0m     embedding_output,\n\u001b[1;32m   1086\u001b[0m     attention_mask,\n\u001b[1;32m   1087\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1088\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1089\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1090\u001b[0m )\n\u001b[1;32m   1091\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:520\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    511\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    512\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    513\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m         rel_embeddings,\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    521\u001b[0m         next_kv,\n\u001b[1;32m    522\u001b[0m         attention_mask,\n\u001b[1;32m    523\u001b[0m         query_states\u001b[38;5;241m=\u001b[39mquery_states,\n\u001b[1;32m    524\u001b[0m         relative_pos\u001b[38;5;241m=\u001b[39mrelative_pos,\n\u001b[1;32m    525\u001b[0m         rel_embeddings\u001b[38;5;241m=\u001b[39mrel_embeddings,\n\u001b[1;32m    526\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    527\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    530\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:362\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    355\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m ):\n\u001b[0;32m--> 362\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    363\u001b[0m         hidden_states,\n\u001b[1;32m    364\u001b[0m         attention_mask,\n\u001b[1;32m    365\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    366\u001b[0m         query_states\u001b[38;5;241m=\u001b[39mquery_states,\n\u001b[1;32m    367\u001b[0m         relative_pos\u001b[38;5;241m=\u001b[39mrelative_pos,\n\u001b[1;32m    368\u001b[0m         rel_embeddings\u001b[38;5;241m=\u001b[39mrel_embeddings,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    371\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:293\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    286\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    292\u001b[0m ):\n\u001b[0;32m--> 293\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    294\u001b[0m         hidden_states,\n\u001b[1;32m    295\u001b[0m         attention_mask,\n\u001b[1;32m    296\u001b[0m         output_attentions,\n\u001b[1;32m    297\u001b[0m         query_states\u001b[38;5;241m=\u001b[39mquery_states,\n\u001b[1;32m    298\u001b[0m         relative_pos\u001b[38;5;241m=\u001b[39mrelative_pos,\n\u001b[1;32m    299\u001b[0m         rel_embeddings\u001b[38;5;241m=\u001b[39mrel_embeddings,\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    302\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:724\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    722\u001b[0m     scale_factor \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    723\u001b[0m scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mtensor(query_layer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat) \u001b[38;5;241m*\u001b[39m scale_factor)\n\u001b[0;32m--> 724\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(query_layer, key_layer\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m scale\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mquery_layer\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention:\n\u001b[1;32m    726\u001b[0m     rel_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_dropout(rel_embeddings)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/kd/Documents/pii_detection/models//deberta_large_mixtral_4_epochs/tokenizer_config.json',\n",
       " '/home/kd/Documents/pii_detection/models//deberta_large_mixtral_4_epochs/special_tokens_map.json',\n",
       " '/home/kd/Documents/pii_detection/models//deberta_large_mixtral_4_epochs/spm.model',\n",
       " '/home/kd/Documents/pii_detection/models//deberta_large_mixtral_4_epochs/added_tokens.json',\n",
       " '/home/kd/Documents/pii_detection/models//deberta_large_mixtral_4_epochs/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(OUTPUT_DIR + \"/deberta_large_mixtral_3_epochs\")\n",
    "tokenizer.save_pretrained(OUTPUT_DIR + \"/deberta_large_mixtral_3_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
